{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzKuBzpO8Zh4",
        "outputId": "2fce51db-cde9-4496-f6f9-52120f86d268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 1000) 크기 행렬을 자기 자신과 200번 곱했을 때 걸리는 시간:\n",
            "CPU: 8.204607725143433 초\n",
            "GPU 없음\n"
          ]
        }
      ],
      "source": [
        "# 즉시 실행 : 그래프를 생성하지 않고 함수를 바로 실행하는 명령형 프로그래밍 환경\n",
        "# 직관적인 인터페이스 - 코드를 자연스럽게 구조화하고 파이썬의 데이터 구조를 활용, 작은 모델과 작은 데이터를 빠르게 반복\n",
        "# 손쉬운 디버깅 - 실행중인 모델의 검토나 변경 사항의 테스트를 위해서 연산을 직접 호출, 에러 확인을 위한 표준 파이썬 디버깅 툴 사용\n",
        "# 자연스러운 흐름 제어 - 그래프 제어 흐름 대신 파이썬 제어 흐름을 사용함으로써 동적인 모델 구조의 단순화\n",
        "\n",
        "\n",
        "# 설치와 기본 사용법\n",
        "import tensorflow as tf\n",
        "import cProfile\n",
        "\n",
        "# tensorflow 2.0에서 즉시 실행은 기본적으로 활성화\n",
        "# tf.executing_eagerly()\n",
        "\n",
        "# tensorflow 연산을 바로 실행할 수 있고, 즉시 결과 확인 가능\n",
        "# x = [[2.]]\n",
        "# m = tf.matmul(x, x)\n",
        "# print(\"hello, {}\".format(m))\n",
        "\n",
        "# a = tf.constant([[1, 2],\n",
        "#                  [3, 4]])\n",
        "# print(a)\n",
        "\n",
        "# 브로드캐스팅 지원\n",
        "# b = tf.add(a, 1)\n",
        "# print(b)\n",
        "\n",
        "# 연산자 오버로딩 지원\n",
        "# print(a * b)\n",
        "\n",
        "# NumPy 값 사용\n",
        "import numpy as np\n",
        "\n",
        "# c = np.multiply(a, b)\n",
        "# print(c)\n",
        "\n",
        "# 텐서로부터 numpy값 얻기\n",
        "# print(a.numpy())\n",
        "\n",
        "\n",
        "# 동적인 제어 흐름\n",
        "# def fizzbuzz(max_num):\n",
        "#     counter = tf.constant(0)\n",
        "#     max_num = tf.convert_to_tensor(max_num)\n",
        "#     for num in range(1, max_num.numpy() + 1):\n",
        "#         num = tf.constant(num)\n",
        "#         if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "#             print(\"FizzBuzz\")\n",
        "#         elif int(num % 3) == 0:\n",
        "#             print(\"Fizz\")\n",
        "#         elif int(num % 5) == 0:\n",
        "#             print(\"Buzz\")\n",
        "#         else:\n",
        "#             print(num.numpy())\n",
        "#         counter+=1\n",
        "\n",
        "# fizzbuzz(15)\n",
        "\n",
        "\n",
        "# 즉시 훈련\n",
        "## 그래디언트 계산 (tf.GradientTape 사용)\n",
        "# w = tf.Variable([[1.0]])\n",
        "# with tf.GradientTape() as tape:\n",
        "#     loss = w * w\n",
        "\n",
        "# grad = tape.gradient(loss, w)\n",
        "# print(grad)\n",
        "\n",
        "## 모델 훈련\n",
        "# mnist 데이터 로드 및 포맷 설정\n",
        "# (mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#     (tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32),\n",
        "#      tf.cast(mnist_labels, tf.int64)))\n",
        "# dataset = dataset.shuffle(1000).batch(32)\n",
        "\n",
        "# 모델 생성\n",
        "# mnist_model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Conv2D(16, [3, 3], activation='relu',\n",
        "#                            input_shape=(None, None, 1)),\n",
        "#     tf.keras.layers.Conv2D(16, [3, 3], activation='relu'),\n",
        "#     tf.keras.layers.GlobalAveragePooling2D(),\n",
        "#     tf.keras.layers.Dense(10)\n",
        "# ])\n",
        "\n",
        "# 즉시 실행에서는 훈련없이도 모델을 사용하고 결과를 점검할 수 있음\n",
        "# for images, labels in dataset.take(1):\n",
        "#     print(\"Logit: \", mnist_model(images[0:1]).numpy())\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam()\n",
        "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# loss_history = []\n",
        "\n",
        "# def train_step(images, labels):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         logits = mnist_model(images, training=True)\n",
        "\n",
        "#         # 결과의 형태를 확인하기 위해 단언문 추가\n",
        "#         tf.debugging.assert_equal(logits.shape, (32, 10))\n",
        "\n",
        "#         loss_value = loss_object(labels, logits)\n",
        "\n",
        "#     loss_history.append(loss_value.numpy().mean())\n",
        "#     grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
        "#     optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
        "\n",
        "# def train():\n",
        "#     for epoch in range(3):\n",
        "#         for (batch, (images, labels)) in enumerate(dataset):\n",
        "#             train_step(images, labels)\n",
        "#         print(\"Epoch {} 종료\".format(epoch))\n",
        "\n",
        "# train()\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(loss_history)\n",
        "# plt.xlabel(\"Batch #\")\n",
        "# plt.ylabel(\"Loss [entropy]\")\n",
        "\n",
        "## 변수와 옵티마이저\n",
        "# class Model(tf.keras.Model):\n",
        "#     def __init__(self):\n",
        "#         super(Model, self).__init__()\n",
        "#         self.W = tf.Variable(5., name='weight')\n",
        "#         self.B = tf.Variable(10., name='bias')\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         return inputs * self.W + self.B\n",
        "\n",
        "# 약 3 * x + 2개의 점으로 구성된 실험 데이터\n",
        "# NUM_EXAMPLES = 2000\n",
        "# training_inputs = tf.random.normal([NUM_EXAMPLES])\n",
        "# noise = tf.random.normal([NUM_EXAMPLES])\n",
        "# training_outputs = training_inputs * 3 + 2 + noise\n",
        "\n",
        "# 최적화할 손실 함수\n",
        "# def loss(model, inputs, targets):\n",
        "#     error = model(inputs) - targets\n",
        "#     return tf.reduce_mean(tf.square(error))\n",
        "\n",
        "# def grad(model, inputs, targets):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         loss_value = loss(model, inputs, targets)\n",
        "#     return tape.gradient(loss_value, [model.W, model.B])\n",
        "\n",
        "# 정의 :\n",
        "# 1. 모델\n",
        "# 2. 모델 파라미터에 대한 손실 함수의 미분\n",
        "# 3. 미분에 기초한 변수 업데이트 전략\n",
        "# model = Model()\n",
        "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# print(\"초기 손실: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
        "\n",
        "# 반복 훈련\n",
        "# for i in range(300):\n",
        "#     grads = grad(model, training_inputs, training_outputs)\n",
        "#     optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
        "#     if i % 20 == 0:\n",
        "#         print(\"Step {:03d}에서 손실: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
        "\n",
        "# print(\"최종 손실: {:3f}\".format(loss(model, training_inputs, training_outputs)))\n",
        "# print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n",
        "\n",
        "\n",
        "# 즉시 실행에서 상태를 위한 객체 사용\n",
        "## 변수는 객체\n",
        "# if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "#     with tf.device(\"gpu:0\"):\n",
        "#         print(\"GPU 사용 가능\")\n",
        "#         v = tf.Variable(tf.random.normal([1000, 1000]))\n",
        "        # v = None                                # v는 더 이상 GPU 메모리를 사용하지 않음\n",
        "\n",
        "## 객체 기반 저장\n",
        "# x = tf.Variable(10.)\n",
        "# checkpoint = tf.train.Checkpoint(x=x)\n",
        "\n",
        "# x.assigh(2.)                                    # 변수에 새로운 값을 할당하고 저장\n",
        "# checkpoint_path = \"./ckpt/\"\n",
        "# checkpoint.save(\"./ckpt/\")\n",
        "\n",
        "# x.assgin(11.)                                   # 저장한 후 변수 변경\n",
        "\n",
        "# 체크포인트로부터 값 복구\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
        "\n",
        "# print(x)\n",
        "\n",
        "# import os\n",
        "\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Conv2D(16, [3, 3], activation='relu'),\n",
        "#     tf.keras.layers.GlobalAveragePooling2D(),\n",
        "#     tf.keras.layers.Dense(10)\n",
        "# ])\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# checkpoint_dir = \"path/to/model_dir\"\n",
        "# if not os.path.exists(checkpoint_dir):\n",
        "#     os.makedirs(checkpoint_dir)\n",
        "\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# root = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                            model=model)\n",
        "# root.save(checkpoint_prefix)\n",
        "# root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "## 객체 지향형 지표\n",
        "# m = tf.keras.metrics.Mean(\"loss\")\n",
        "# m(0)\n",
        "# m(5)\n",
        "# m.result()\n",
        "# m([8, 9])\n",
        "# m.result()\n",
        "\n",
        "\n",
        "# summary와 TensorBoard\n",
        "# TensorBoard : 훈련 과정에서 모델을 파악하거나 디버깅하고 최적화하기 위해 사용하는 시각화 도구\n",
        "# TensorBoard는 프로그램이 실행되는 동안 작성된 summary 이벤트를 사용\n",
        "# logdir = \"./tb/\"\n",
        "# writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "# with writer.as_default():                       # 또는 반복 전 writer.set_as_default()를 호출\n",
        "#     for i in range(1000):\n",
        "#         step = i + 1\n",
        "#         # 실제 훈련 함수로 손실 계산\n",
        "#         loss = 1 - 0.001 * step\n",
        "#         if step % 100 == 0:\n",
        "#             tf.summary.scalar('손실', loss, step=step)\n",
        "\n",
        "# !ls tb/\n",
        "\n",
        "# 자동 미분 관련 고급편\n",
        "## 동적 모델\n",
        "# def line_search_step(fn, init_x, rate=1.0):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         # 변수는 자동적으로 기록되지만 텐서는 사용자 스스로 확인해야 함\n",
        "#         tape.watch(init_x)\n",
        "#         value = fn(init_x)\n",
        "\n",
        "#     grad = tape.gradient(value, init_x)\n",
        "#     grad_norm = tf.reduce_sum(grad * grad)\n",
        "#     init_value = value\n",
        "#     while value > init_value - rate * grad_norm:\n",
        "#         x = init_x - rate * grad\n",
        "#         value = fn(x)\n",
        "#         rate /= 2.0\n",
        "#     return x, value\n",
        "\n",
        "## 사용자 정의 그래디언트\n",
        "# 정방향 함수 안에서 입력값 또는 출력값, 중간값과 관련된 그래디언트를 정의해야 함\n",
        "# @tf.custom_gradient\n",
        "# def clip_gradient_by_norm(x, norm):\n",
        "#     y = tf.identify(x)\n",
        "#     def grad_fn(dresult):\n",
        "#         return [tf.clip_by_norm(dresult, norm), None]\n",
        "#     return y, grad_fn\n",
        "\n",
        "# 사용자 정의 그래디언트는 일반적으로 연산에 대해 수치적으로 안정된 그래디언트를 제공하기 위해 사용\n",
        "# def log1pexp(x):\n",
        "#     return tf.math.log(1 + tf.exp(x))\n",
        "\n",
        "# def grad_log1pexp(x):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         tape.watch(x)\n",
        "#         value = log1pexp(x)\n",
        "#     return tape.gradient(value, x)\n",
        "\n",
        "# 그래디언트 계산은 x = 0일 때 잘 동작함\n",
        "# grad_log1pexp(tf.constant(0.)).numpy()\n",
        "\n",
        "# 그러나, x = 100일 때 수치적으로 불안정하기 때문에 실패\n",
        "# grad_log1pexp(tf.constant(100.)).numpy()\n",
        "\n",
        "# 불필요한 계산을 제거함으로써 계산을 효율적으로 하기 위해 정방향 경로 안에서 계산된 tf.exp(x) 값을 재사용\n",
        "# @tf.custom_gradient\n",
        "# def log1pexp(x):\n",
        "#     e = tf.exp(x)\n",
        "#     def grad(dy):\n",
        "#         return dy * (1 - 1 / (1 + e))\n",
        "#     return tf.math.log(1 + e), grad\n",
        "\n",
        "# def grad_log1pexp(x):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         tape.watch(x)\n",
        "#         value = log1pexp(x)\n",
        "#     return tape.gradient(value, x)\n",
        "\n",
        "# 전처럼, 그래디언트 계산은 x = 0일 때 잘 동작\n",
        "# grad_log1pexp(tf.constant(0.)).numpy()\n",
        "\n",
        "# 그래디언트 계산은 x = 100일 때 역시 잘 동작\n",
        "# grad_log1pexp(tf.constant(100.)).numpy()\n",
        "\n",
        "\n",
        "# 성능\n",
        "import time\n",
        "\n",
        "def measure(x, steps):\n",
        "    # 텐서플로는 처음 사용할 때 GPU를 초기화, 시간 계산에서 제외\n",
        "    tf.matmul(x, x)\n",
        "    start = time.time()\n",
        "    for i in range(steps):\n",
        "        x = tf.matmul(x, x)\n",
        "\n",
        "    # tf.matmul는 행렬 곱셉을 완료하기 전에 결과 반환 가능\n",
        "    # 아래 x.numpy() 호출은 대기열에 추가된 모든 연산이 완료된 것임을 보장\n",
        "    # (그리고 그 결과가 호스트 메모리에 복사되므로 matmul 연산 시간은 조금 많은 연산 시간이 포함됨)\n",
        "\n",
        "    _ = x.numpy()\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "shape = (1000, 1000)\n",
        "steps = 200\n",
        "print(\"{} 크기 행렬을 자기 자신과 {}번 곱했을 때 걸리는 시간:\".format(shape, steps))\n",
        "\n",
        "# CPU에서 실행\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    print(\"CPU: {} 초\".format(measure(tf.random.normal(shape), steps)))\n",
        "\n",
        "# GPU에서 실행(가능하다면)\n",
        "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        print(\"GPU: {} 초\".format(measure(tf.random.normal(shape), steps)))\n",
        "else:\n",
        "    print(\"GPU 없음\")\n",
        "\n",
        "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
        "    x = tf.random.normal([10, 10])\n",
        "\n",
        "    x_gpu0 = x.gpu()\n",
        "    x_cpu = x.cpu()\n",
        "\n",
        "    _ = tf.matmul(x_cpu, x_cpu)                 # CPU에서 실행\n",
        "    _ = tf.matmul(x_gpu0, x_gpu0)               # GPU:0에서 실행"
      ]
    }
  ]
}
