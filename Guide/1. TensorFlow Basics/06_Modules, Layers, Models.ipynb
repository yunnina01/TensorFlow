{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8giitm4DnpY"
      },
      "outputs": [],
      "source": [
        "# 모듈, 레이어 및 모델\n",
        "# 모델은 추상적으로 텐서에서 무언가를 계산하는 함수(정방향 전달)나 훈련에 대한 응답으로 업데이트할 수 있는 일부 변수\n",
        "\n",
        "\n",
        "# TensorFlow에서 모델 및 레이어 정의\n",
        "# 대부분의 모델은 레이어(재사용할 수 있고 훈련 가능한 변수를 가진, 알려진 수학적 구조의 함수)로 구성\n",
        "# TensorFlow에서 Keras 또는 Sonnet과 같은 레이어 및 모델의 상위 수준 구현 대부분은 같은 기본 클래스인 tf.Module를 기반으로 구축\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "\n",
        "# %load_ext tensorboard\n",
        "\n",
        "# class SimpleModule(tf.Module):\n",
        "#     def __init__(self, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.a_variable = tf.Variable(5.0, name='train_me')\n",
        "#         self.non_trainable_variable = tf.Variable(5.0, trainable=False, name='do_not_train_me')\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         return self.a_variable * x + self.non_trainable_variable\n",
        "\n",
        "# simple_module = SimpleModule(name='simple')\n",
        "# simple_module(tf.constant(5.0))\n",
        "\n",
        "# 모듈과 더 나아가 레이어는 '객체'에 대한 딥러닝 용어이며, 내부 상태와 해당 상태를 사용하는 메서드가 존재\n",
        "# 미세 조정 중 레이어 및 변수 고정을 포함하여 어떤 이유로든 변수의 훈련 가능성을 설정 및 해제 가능\n",
        "# tf.Module을 하위 클래스화함으로써 이 객체의 속성에 할당된 tf.Variable 또는 tf.Module 인스턴스가 자동으로 수집\n",
        "# 이를 통해 변수를 저장 및 로드할 수 있으며, tf.Module 모음을 만들 수 있음\n",
        "# print(\"Trainable variables:\", simple_module.trainable_variables)\n",
        "# print(\"All variables:\", simple_module.variables)\n",
        "\n",
        "# 밀집(선형) 레이어\n",
        "# class Dense(tf.Module):\n",
        "#     def __init__(self, in_features, out_features, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
        "#         self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         y = tf.matmul(x, self.w) + self.b\n",
        "#         return tf.nn.relu(y)\n",
        "\n",
        "# 2개의 레이어 인스턴스를 만들고 적용하는 전체 모델\n",
        "# class SequentialModule(tf.Module):\n",
        "#     def __init__(self, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.dense_1 = Dense(in_features=3, out_features=3)\n",
        "#         self.dense_2 = Dense(in_features=3, out_features=2)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         x = self.dense_1(x)\n",
        "#         return self.dense_2(x)\n",
        "\n",
        "# my_model = SequentialModule(name='the_model')\n",
        "# print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n",
        "# print(\"Submodules:\", my_model.submodules)\n",
        "\n",
        "# for var in my_model.variables:\n",
        "#     print(var, \"\\n\")\n",
        "\n",
        "## 변수 생성 연기\n",
        "# 특정 입력 형상으로 모듈이 처음 호출될 때까지 변수 생성을 연기하면 입력 크기를 미리 지정할 필요가 없음\n",
        "# class FlexibleDenseModule(tf.Module):\n",
        "#     def __init__(self, out_features, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.is_built = False\n",
        "#         self.out_features = out_features\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         # Create variables on first call\n",
        "#         if not self.is_built:\n",
        "#             self.w = tf.Variable(tf.random.normal([x.shape[-1], self.out_features]), name='w')\n",
        "#             self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
        "#             self.is_built = True\n",
        "\n",
        "#         y = tf.matmul(x, self.w) + self.b\n",
        "#         return tf.nn.relu(y)\n",
        "\n",
        "# class MySequentialModule(tf.Module):\n",
        "#     def __init__(self, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.dense_1 = FlexibleDenseModule(out_features=3)\n",
        "#         self.dense_2 = FlexibleDenseModule(out_features=2)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         x = self.dense_1(x)\n",
        "#         return self.dense_2(x)\n",
        "\n",
        "# my_model = MySequentialModule(name='the_model')\n",
        "# print(\"Model results:\", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n",
        "\n",
        "\n",
        "# 가중치 저장\n",
        "# tf.Module은 checkpoint와 SavedModel로 모두 저장 가능\n",
        "# 체크포인트는 가중치(모듈 및 하위 모듈 내부의 변수 세트 값)\n",
        "# chkp_path = \"my_checkpoint\"\n",
        "# checkpoint = tf.train.Checkpoint(model=my_model)\n",
        "# checkpoint.write(chkp_path)\n",
        "\n",
        "# 체크포인트는 데이터 자체와 메타데이터용 인덱스 파일로 구성\n",
        "# 인덱스 파일은 실제로 저장된 항목과 체크포인트 번호를 추적하는 반면 체크포인트 데이터에는 변수 값과 해당 속성 조회 경로가 포함\n",
        "# !ls my_checkpoint*\n",
        "\n",
        "# tf.train.list_variables(chkp_path)\n",
        "\n",
        "# 분산 훈련 중에 변수 모음이 샤딩될 수 있으므로 번호가 매겨짐(예: '00000-of-00001')\n",
        "# 이 경우에는 샤드가 하나만 존재\n",
        "# new_model = MySequentialModule()\n",
        "# new_checkpoint = tf.train.Checkpoint(model=new_model)\n",
        "# new_checkpoint.restore(\"my_checkpoint\")\n",
        "\n",
        "# Should be the same result as above\n",
        "# new_model(tf.constant([[2.0, 2.0, 2.0]]))\n",
        "\n",
        "\n",
        "# 함수 저장\n",
        "# class Dense(tf.Module):\n",
        "#     def __init__(self, in_features, out_features, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
        "#         self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         y = tf.matmul(x, self.w) + self.b\n",
        "#         return tf.nn.relu(y)\n",
        "\n",
        "# class MySequentialModule(tf.Module):\n",
        "#     def __init__(self, name=None):\n",
        "#         super().__init__(name=name)\n",
        "#         self.dense_1 = Dense(in_features=3, out_features=3)\n",
        "#         self.dense_2 = Dense(in_features=3, out_features=2)\n",
        "\n",
        "#     @tf.function\n",
        "#     def __call__(self, x):\n",
        "#         x = self.dense_1(x)\n",
        "#         return self.dense_2(x)\n",
        "\n",
        "# A model with a graph\n",
        "# my_model = MySequentialModule(name='the_model')\n",
        "\n",
        "# print(my_model([[2.0, 2.0, 2.0]]))\n",
        "# print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n",
        "\n",
        "# TensorBoard 요약 내에서 그래프를 추적해 그래프 시각화\n",
        "# Set up logging\n",
        "# stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# logdir = \"logs/func/%s\" % stamp\n",
        "# writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "# Create a new model to get a fresh trace otherwise the summary will not see the graph\n",
        "# new_model = MySequentialModule()\n",
        "\n",
        "# Bracket the function call with tf.summary.trace_on() an tf.summary.trace_export()\n",
        "# tf.summary.trace_on(graph=True)\n",
        "# tf.profiler.experimental.start(logdir)\n",
        "\n",
        "# Call only one tf.function when tracing\n",
        "# z = print(new_model(tf.constant([[2.0, 2.0, 2.0]])))\n",
        "# with writer.as_default():\n",
        "#     tf.summary.trace_export(\n",
        "#         name='my_func_trace',\n",
        "#         step=0,\n",
        "#         profiler_outdir=logdir)\n",
        "\n",
        "# docs_infra: no execute\n",
        "# %tensorboard --logdir logs/func\n",
        "\n",
        "## SavedModel 생성\n",
        "# 완전히 훈련된 모델을 공유하는 권장 방법은 SavedModel을 사용하는 방법\n",
        "# SavedModel에는 함수 모음과 가중치 모음이 모두 포함\n",
        "# tf.saved_model.save(my_model, 'the_saved_model')\n",
        "\n",
        "# saved_model.pb 파일은 함수형 tf.Graph를 설명하는 프로토콜 버퍼\n",
        "# Inspect the SavedModel in the directory\n",
        "# !ls -l the_saved_model\n",
        "\n",
        "# The variables/ directory contains a checkpoint of the variables\n",
        "# !ls -l the_saved_model/variables\n",
        "\n",
        "# new_model = tf.saved_model.load('the_saved_model')\n",
        "\n",
        "# 저장된 모델을 로드하여 생성된 new_model은 클래스 지식이 없는 내부 TensorFlow 사용자 객체\n",
        "# isinstance(new_model, SequentialModule)\n",
        "\n",
        "# 저장된 모델을 로드하여 생성된 모델은 이미 정의된 입력 서명에서 동작, 서명 추가 불가능\n",
        "# print(my_model([[2.0, 2.0, 2.0]]))\n",
        "# print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n",
        "\n",
        "\n",
        "# Keras 모델 및 레이어\n",
        "## Keras 레이어\n",
        "# tf.keras.layers.Layer는 모든 Keras 레이어의 기본 클래스이며, tf.Module을 상속\n",
        "# 부모를 교체한 다음, __call__을 call로 변경하여 모듈을 Keras 레이어로 변환 가능\n",
        "# class MyDense(tf.keras.layers.Layer):\n",
        "#     # Adding **kwargs to support base Keras layer arguments\n",
        "#     def __init__(self, in_features, out_features, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')\n",
        "#         self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
        "\n",
        "#     def call(self, x):\n",
        "#         y = tf.matmul(x, self.w) + self.b\n",
        "#         return tf.nn.relu(y)\n",
        "\n",
        "# simple_layer = MyDense(name='simple', in_features=3, out_features=3)\n",
        "# simple_layer([[2.0, 2.0, 2.0]])\n",
        "\n",
        "## build 단계\n",
        "# 입력 형상이 확실해질 때까지 변수를 생성하기 위해 기다리는 것이 많은 경우 편리\n",
        "# Keras 레이어에는 레이어를 정의하는 방법에 더 많은 유연성을 제공하는 추가 수명 주기 단계가 있고, build 함수에서 정의\n",
        "# build는 정확히 한 번만 호출되며 입력 형상으로 호출됨\n",
        "class FlexibleDense(tf.keras.layers.Layer):\n",
        "    # Note the added '**kwargs', as Keras supports many arguments\n",
        "    def __init__(self, out_features, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.out_features = out_features\n",
        "\n",
        "    # Create the state of the layer(weights)\n",
        "    def build(self, input_shape):\n",
        "        self.w = tf.Variable(tf.random.normal([input_shape[-1], self.out_features]), name='w')\n",
        "        self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
        "\n",
        "    # Defines the computation from inputs to outputs\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# flexible_dense = FlexibleDense(out_features=3)\n",
        "\n",
        "# 이 시점에는 모델이 빌드되지 않았으므로 변수가 없음\n",
        "# flexible_dense.variables\n",
        "\n",
        "# print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))\n",
        "\n",
        "# flexible_dense.variables\n",
        "\n",
        "# build는 한 번만 호출되므로 입력 형상이 레이어의 변수와 호환되지 않으면 입력이 거부됨\n",
        "# try:\n",
        "#     print(\"Model results:\", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]])))\n",
        "# except tf.errors.InvalidArgumentError as e:\n",
        "#     print(\"Failed:\", e)\n",
        "\n",
        "# Keras 레이어에는 다음과 같은 더 많은 추가 기능이 존재\n",
        "# 1. 선택적 손실\n",
        "# 2. 메트릭 지원\n",
        "# 3. 훈련 및 추론 사용을 구분하기 위한 선택적 tracing 인수에 대한 기본 지원\n",
        "# 4. Python에서 모델 복제를 허용하도록 구성을 정확하게 저장할 수 있는 get_config 및 from_config 메서드\n",
        "\n",
        "## Keras 모델\n",
        "# 모델을 중첩된 Keras 레이어로 정의 가능하지만, Keras는 tf.keras.Model이라는 완전한 기능을 갖춘 모델 클래스도 제공\n",
        "# tf.keras.layers.Layer에서 상속되므로 Keras 모델은 Keras 레이어와 마찬가지로 사용, 중첩 및 저장 가능\n",
        "# Keras 모델에는 쉽게 훈련, 평가, 로드 및 저장하고, 심지어 여러 머신에서 훈련할 수 있는 추가 기능 존재\n",
        "class MySequentialModel(tf.keras.Model):\n",
        "    def __init__(self, name=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense_1 = FlexibleDense(out_features=3)\n",
        "        self.dense_2 = FlexibleDense(out_features=2)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        return self.dense_2(x)\n",
        "\n",
        "my_sequential_model = MySequentialModel(name='the_model')\n",
        "\n",
        "# print(\"Model results:\", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))\n",
        "\n",
        "# my_sequential_model.variables\n",
        "# my_sequential_model.submodules\n",
        "\n",
        "# tf.keras.Model을 재정의하는 것은 TensorFlow 모델을 빌드하는 Python다운 접근 방식\n",
        "# 다른 프레임워크에서 마이그레이션할 경우 매우 간단할 가능성\n",
        "# 기존 레이어와 입력을 간단하게 조합한 모델을 구성하는 경우,\n",
        "# 모델 재구성 및 아키텍처와 관련된 추가 기능과 함께 제공되는 함수형 API를 사용해 시간과 공간 절약 가능\n",
        "\n",
        "# 험수형 API가 있는 동일한 모델\n",
        "# 가장 큰 차이점은 입력 형상이 함수형 구성 프로세스의 일부로 미리 지정\n",
        "# 이 경우, input_shape 인수를 완전히 지정할 필요가 없으며, 일부 차원은 None으로 남겨둘 수 있음\n",
        "# inputs = tf.keras.Input(shape=[3,])\n",
        "\n",
        "# x = FlexibleDense(3)(inputs)\n",
        "# x = FlexibleDense(2)(x)\n",
        "\n",
        "# my_functional_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "# my_functional_model.summary()\n",
        "\n",
        "# my_functional_model(tf.constant([[2.0, 2.0, 2.0]]))\n",
        "\n",
        "\n",
        "# Keras 모델 저장\n",
        "# Keras 모델에서는 체크포인트를 사용할 수 있으며, tf.Module과 같게 보임\n",
        "# Keras 모델은 모듈 tf.saved_models.save()로 저장할 수도 있으나 Keras 모델에는 편리한 메서드와 기타 기능이 존재\n",
        "my_sequential_model.save(\"exname_of_file\")\n",
        "reconstructed_model = tf.keras.models.load_model(\"exname_of_file\")\n",
        "\n",
        "# Keras SavedModel는 또한 메트릭, 손실 및 옵티마이저 상태를 저장\n",
        "# 재구성된 모델을 사용할 수 있으며, 같은 데이터로 호출될 때 같은 결과 생성\n",
        "reconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))"
      ]
    }
  ]
}