{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COAC-H_j8ohN",
        "outputId": "14e40967-f6d4-4c25-a1b5-1d41914e1c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparseTensor(indices=tf.Tensor(\n",
            "[[0 0]\n",
            " [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
            "tf.Tensor(\n",
            "[[1 0 0 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 0]], shape=(3, 4), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# 텐서 : 일관된 유형(dtype)을 가진 다차원 배열로 np.arrays와 유사\n",
        "# 모든 dtypes은 tf.dtypes.DType에서 확인 가능\n",
        "# 모든 텐서는 Python 숫자 및 문자열과 같이 변경할 수 없음\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 기초\n",
        "# An int32 tensor by defalut\n",
        "# rank_0_tensor = tf.constant(4)                              # 스칼라(순위-0 텐서) : 단일 값 포함, 축 없음\n",
        "# print(rank_0_tensor)\n",
        "\n",
        "# A float tensor\n",
        "# rank_1_tensor = tf.constant([2.0, 3.0, 4.0])                # 벡터(순위-1 텐서) : 값 목록, 하나의 축\n",
        "# print(rank_1_tensor)\n",
        "\n",
        "# If you want to be specific, you can set the dtype at creation time\n",
        "# rank_2_tensor = tf.constant([[1, 2],\n",
        "#                              [3, 4],\n",
        "#                              [5, 6]], dtype=tf.float16)     # 행렬(순위-2 텐서) : 두 개의 축\n",
        "# print(rank_2_tensor)\n",
        "\n",
        "# There can be an arbitrary number of axes(somtimes called dimensions)\n",
        "# rank_3_tensor = tf.constant([\n",
        "#     [[0, 1, 2, 3, 4],\n",
        "#      [5, 6, 7, 8, 9]],\n",
        "#     [[10, 11, 12, 13, 14],\n",
        "#      [15, 16, 17, 18, 19]],\n",
        "#     [[20, 21, 22, 23, 24],\n",
        "#      [25, 26, 27, 28, 29]],])                               # 세 개의 축이 있는 텐서\n",
        "# print(rank_3_tensor)\n",
        "\n",
        "# np.array 또는 tensor.numpy 메서드를 사용해 텐서를 NumPy 배열로 변환\n",
        "# np.array(rank_2_tensor)\n",
        "# rank_2_tensor.numpy()\n",
        "\n",
        "# 덧셈, 요소별 곱셈 및 행렬 곱셈을 포함한 텐서에 대한 기본 산술 연산 수행 가능\n",
        "# a = tf.constant([[1, 2],\n",
        "#                  [3, 4]])\n",
        "# b = tf.constant([[1, 1],\n",
        "#                  [1, 1]])                                   # tf.ones([2, 2])\n",
        "\n",
        "# print(tf.add(a, b))\n",
        "# print(tf.multiply(a, b))\n",
        "# print(tf.matmul(a, b))\n",
        "\n",
        "# print(a + b)                                                # element-wise addition\n",
        "# print(a * b)                                                # element-wise multiplication\n",
        "# print(a @ b)                                                # matrix multiplication\n",
        "\n",
        "# c = tf.constant([[4.0, 5.0], [10.0, 1.0]])\n",
        "\n",
        "# print(tf.reduce_max(c))                                     # Find the largest value\n",
        "# print(tf.math.argmax(c))                                    # Find the index of the largest value\n",
        "# print(tf.nn.softmax(c))                                     # Compute the softmax\n",
        "\n",
        "# tf.convert_to_tensor([1, 2, 3])\n",
        "# tf.reduce_max([1, 2, 3])\n",
        "# tf.reduce_max(np.array([1, 2, 3]))\n",
        "\n",
        "\n",
        "# 형상 정보\n",
        "# 형상 : 텐서의 각 차원의 길이(요소의 수)\n",
        "# 순위 : 텐서 축의 수 (스칼라 : 0, 벡터 : 1, 행렬 : 2)\n",
        "# 축 또는 차원 : 텐서의 특정 차원\n",
        "# 크기 : 텐서의 총 항목 수, 형상 벡터 요소의 곱\n",
        "# rank_4_tensor = tf.zeros([3, 2, 4, 5])\n",
        "\n",
        "# print(\"Type of every element:\", rank_4_tensor.dtype)\n",
        "# print(\"Number of axes:\", rank_4_tensor.ndim)\n",
        "# print(\"Shape of tensor:\", rank_4_tensor.shape)\n",
        "# print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\n",
        "# print(\"Elements along the last axis of tensor:\", rank_4_tensor.shape[-1])\n",
        "# print(\"Total number of elements (3 * 2 * 4 * 5):\", tf.size(rank_4_tensor).numpy())\n",
        "\n",
        "# Tensor.ndim 및 Tensor.shape 속성은 Tensor 객체를 반환하지 않음\n",
        "# 따라서, Tensor가 필요한 경우 tf.rank 또는 tf.shape 함수를 사용\n",
        "# tf.rank(rank_4_tensor)\n",
        "# tf.shape(rank_4_tensor)\n",
        "\n",
        "\n",
        "# 인덱싱\n",
        "## 단일 축 인덱싱\n",
        "# rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
        "# print(rank_1_tensor.numpy())\n",
        "\n",
        "# print(\"First:\", rank_1_tensor[0].numpy())\n",
        "# print(\"Second:\", rank_1_tensor[1].numpy())\n",
        "# print(\"Last:\", rank_1_tensor[-1].numpy())\n",
        "\n",
        "# print(\"Everything:\", rank_1_tensor[:].numpy())\n",
        "# print(\"Before 4:\", rank_1_tensor[:4].numpy())\n",
        "# print(\"From 4 to the end:\", rank_1_tensor[4:].numpy())\n",
        "# print(\"From 2, before 7:\", rank_1_tensor[2:7].numpy())\n",
        "# print(\"Every other item:\", rank_1_tensor[::2].numpy())\n",
        "# print(\"Reversed:\", rank_1_tensor[::-1].numpy())\n",
        "\n",
        "## 다축 인덱싱\n",
        "# rank_2_tensor = tf.constant([[1, 2],\n",
        "#                              [3, 4],\n",
        "#                              [5, 6]], dtype=tf.float16)\n",
        "# print(rank_2_tensor.numpy())\n",
        "\n",
        "# Pull out a single value from a 2-rank tensor\n",
        "# print(rank_2_tensor[1, 1].numpy())\n",
        "\n",
        "# Get row and column tensors\n",
        "# print(\"Second row:\", rank_2_tensor[1, :].numpy())\n",
        "# print(\"Second column:\", rank_2_tensor[:, 1].numpy())\n",
        "# print(\"Last row:\", rank_2_tensor[-1, :].numpy())\n",
        "# print(\"First item in last column:\", rank_2_tensor[0, -1].numpy())\n",
        "# print(\"Skip the first row:\\n\", rank_2_tensor[1:, :].numpy())\n",
        "\n",
        "\n",
        "# 형상 조작\n",
        "# Shape returns a 'TensorShape' object that shows the size along each axis\n",
        "# x = tf.constant([[1], [2], [3]])\n",
        "# print(x.shape)\n",
        "\n",
        "# You can convert this object into a Python list\n",
        "# print(x.shape.as_list())\n",
        "\n",
        "# You can reshape a tensor to a new shape\n",
        "# Note that you're passing in a list\n",
        "# reshape = tf.reshape(x, [1, 3])\n",
        "# print(x.shape)\n",
        "# print(reshape.shape)\n",
        "\n",
        "# rank_3_tensor = tf.constant([\n",
        "#     [[0, 1, 2, 3, 4],\n",
        "#      [5, 6, 7, 8, 9]],\n",
        "#     [[10, 11, 12, 13, 14],\n",
        "#      [15, 16, 17, 18, 19]],\n",
        "#     [[20, 21, 22, 23, 24],\n",
        "#      [25, 26, 27, 28, 29]],])\n",
        "\n",
        "# A '-1' passed in the 'shape' argument says \"Whatever fits\"\n",
        "# print(tf.reshape(rank_3_tensor, [-1]))\n",
        "\n",
        "# 일반적으로 tf.reshape의 합리적인 용도는 인접한 축을 결합하거나 분할하는 것뿐임 (또는 1을 추가/제거)\n",
        "# print(tf.reshape(rank_3_tensor, [3 * 2, 5]))\n",
        "# print(tf.reshape(rank_3_tensor, [3, 2 * 5]))\n",
        "\n",
        "# tf.reshape에서 축 교환이 작동하지 않으면, tf.transpose를 수행\n",
        "# Bad examples\n",
        "# You can't reorder axes with reshape\n",
        "# print(tf.reshape(rank_3_tensor, [2, 3, 5]))\n",
        "\n",
        "# This is a mess\n",
        "# print(tf.reshape(rank_3_tensor, [5, 6]))\n",
        "\n",
        "# This doesn't work at all\n",
        "# try:\n",
        "#     tf.reshape(rank_3_tensor, [7, -1])\n",
        "# except Exception as e:\n",
        "#     print(f\"{type(e).__name__}: {e}\")\n",
        "\n",
        "\n",
        "# DTypes에 대한 추가 정보\n",
        "# tf.tensor의 데이터 유형은 Tensor.dtype 속성을 사용해 검사\n",
        "# Python 객체에서 tf.Tensor를 만들 때 선택적으로 데이터 유형 지정 가능\n",
        "# the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)\n",
        "# the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)\n",
        "\n",
        "# Now, cast to an uint8 and lose the decimal precision\n",
        "# the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)\n",
        "# print(the_u8_tensor)\n",
        "\n",
        "\n",
        "# 브로드캐스팅 : 특정 조건에서 작은 텐서는 결합된 연산을 실행할 때 더 큰 텐서에 맞게 자동으로 확장\n",
        "# x = tf.constant([1, 2, 3])\n",
        "# y = tf.constant(2)\n",
        "# z = tf.constant([2, 2, 2])\n",
        "\n",
        "# All of these are the same computation\n",
        "# print(tf.multiply(x, 2))\n",
        "# print(x * y)\n",
        "# print(x * z)\n",
        "\n",
        "# These are the same computations\n",
        "# x = tf.reshape(x, [3, 1])\n",
        "# y = tf.range(1, 5)\n",
        "\n",
        "# print(x)\n",
        "# print(y)\n",
        "# print(tf.multiply(x, y))\n",
        "\n",
        "# x_stretch = tf.constant([[1, 1, 1, 1],\n",
        "#                          [2, 2, 2, 2],\n",
        "#                          [3, 3, 3, 3]])\n",
        "# y_stretch = tf.constant([[1, 2, 3, 4],\n",
        "#                          [1, 2, 3, 4],\n",
        "#                          [1, 2, 3, 4]])\n",
        "# print(x_stretch * y_stretch)                                # Again, operator overloading\n",
        "\n",
        "# tf.broadcast_to를 사용해 브로드캐스팅이 어떤 모습인지 확인 가능\n",
        "# print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n",
        "\n",
        "\n",
        "# tf.convert_to_tensor\n",
        "# 대부분의 ops는 텐서가 아닌 인수에 대해 convert_to_tensor를 호출\n",
        "# 변환 레지스트리가 있어 NumPy의 ndarray, TensorShape, Python 목록 및 tf.Variable과 같은 대부분의 객체 클래스는 모두 자동으로 변환\n",
        "\n",
        "\n",
        "# 비정형(ragged) 텐서 : 어떤 축을 따라 다양한 수의 요소를 가진 텐서\n",
        "# 비정형 데이터에는 tf.ragged.RaggedTensor를 사용\n",
        "# ragged_list = [\n",
        "#     [0, 1, 2, 3],\n",
        "#     [4, 5],\n",
        "#     [6, 7, 8],\n",
        "#     [9]]\n",
        "\n",
        "# try:\n",
        "#     tensor = tf.constant(ragged_list)\n",
        "# except Exception as e:\n",
        "#     print(f\"{type(e).__name__}: {e}\")\n",
        "\n",
        "# ragged_tensor = tf.ragged.constant(ragged_list)\n",
        "# print(ragged_tensor)\n",
        "# print(ragged_tensor.shape)\n",
        "\n",
        "\n",
        "# 문자열 텐서\n",
        "# tf.string은 dtype이며, 텐서에서 문자열(가변 길이의 바이트 배열)과 같은 데이터를 나타낼 수 있음\n",
        "# 문자열은 원자성이므로 Python 문자열과 같은 방식으로 인덱싱 할 수 없음\n",
        "# scalar_string_tensor = tf.constant(\"Gray wolf\")\n",
        "# print(scalar_string_tensor)\n",
        "\n",
        "# If you have three string tensors of different lengths, this is OK\n",
        "# tensor_of_strings = tf.constant([\"Gray wolf\",\n",
        "#                                  \"Quick brown fox\",\n",
        "#                                  \"Lazy dog\"])\n",
        "\n",
        "# Note that the shape is (3,). The string length is not included\n",
        "# print(tensor_of_strings)\n",
        "\n",
        "# b 접두사는 tf.string dtype이 유니코드 문자열이 아닌 바이트 문자열임을 나타냄\n",
        "# 유니코드 문자를 전달하면 UTF-8로 인코딩\n",
        "# tf.constant(\"🥳👍\")\n",
        "\n",
        "# You can use split to split a string into a set of tensors\n",
        "# print(tf.strings.split(scalar_string_tensor, sep=\" \"))\n",
        "\n",
        "# 하지만, 문자열 텐서를 분할하면 비정형 텐서로 변함\n",
        "# 각 문자열이 서로 다른 수의 부분으로 분할될 수 있기 때문\n",
        "# print(tf.strings.split(tensor_of_strings))\n",
        "\n",
        "# text = tf.constant(\"1 10 100\")\n",
        "# print(tf.strings.to_number(tf.strings.split(text, \" \")))\n",
        "\n",
        "# tf.cast를 사용해 문자열 텐서를 숫자로 변환할 수는 없지만, 바이트로 변환한 다음 숫자로 변환 가능\n",
        "# byte_strings = tf.strings.bytes_split(tf.constant(\"Duck\"))\n",
        "# byte_ints = tf.io.decode_raw(tf.constant(\"Duck\"), tf.uint8)\n",
        "\n",
        "# print(\"Byte strings:\", byte_strings)\n",
        "# print(\"Bytes:\", byte_ints)\n",
        "\n",
        "# Or split it up as unicode and then decode it\n",
        "# unicode_bytes = tf.constant(\"アヒル 🦆\")\n",
        "# unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, \"UTF-8\")\n",
        "# unicode_values = tf.strings.unicode_decode(unicode_bytes, \"UTF-8\")\n",
        "\n",
        "# print(\"Unicode bytes:\", unicode_bytes)\n",
        "# print(\"Unicode chars:\", unicode_char_bytes)\n",
        "# print(\"Unicode values:\", unicode_values)\n",
        "\n",
        "\n",
        "# 희소 텐서\n",
        "# TensorFlow는 tf.sparse.SparseTensor 및 관련 연산을 지원해 희소 데이터를 효율적으로 저장\n",
        "# 희소 텐서는 메모리 효율적인 방식으로 인덱스별로 값을 저장\n",
        "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
        "                                       values=[1, 2],\n",
        "                                       dense_shape=[3, 4])\n",
        "print(sparse_tensor)\n",
        "\n",
        "# You can convert sparse tensors to dense\n",
        "print(tf.sparse.to_dense(sparse_tensor))"
      ]
    }
  ]
}