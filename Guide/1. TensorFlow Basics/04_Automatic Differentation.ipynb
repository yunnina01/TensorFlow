{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "46UqEWfk0lIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19629b1d-3531-492d-f180-286694b7906b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# 자동 미분 및 그래디언트\n",
        "# 자동 미분은 신경망 학습을 위한 역전파와 같은 머신러닝 알고리즘을 구현하는 데 유용\n",
        "\n",
        "\n",
        "# 그래디언트 계산\n",
        "# 자동 미분을 위해 TensorFlow는 정방향 패스 동안 어떤 연산이 어떤 순서로 발생하는지 기억\n",
        "# 그 후, 역방향 패스 동안 TensorFlow는 이 연산 목록을 역순으로 이동하여 그래디언트 계산\n",
        "\n",
        "\n",
        "# 그래디언트 테이프 : TensorFlow에서 자동 미분을 위해 제공되는 API\n",
        "# tf.GradientTape는 컨텍스트 안에서 실행된 모든 연산을 테이프에 기록\n",
        "# 그 다음, TensorFlow는 후진 방식 자동 미분을 사용해 테이프에 기록된 연산의 그래디언트 계산\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# x = tf.Variable(3.0)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     y = x ** 2\n",
        "\n",
        "# 일부 연산을 기록한 후, GradientTape.gradient(target, source)를 사용해 그래디언트 계산\n",
        "# dy_dx = tape.gradient(y, x)\n",
        "# dy_dx.numpy()\n",
        "\n",
        "# w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "# b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "# x = [[1., 2., 3.]]\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     y = x @ w + b\n",
        "#     loss = tf.reduce_mean(y ** 2)\n",
        "\n",
        "# [dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
        "\n",
        "# print(w.shape)\n",
        "# print(dl_dw.shape)\n",
        "\n",
        "# my_vars = {\n",
        "#     'w': w,\n",
        "#     'b': b\n",
        "# }\n",
        "\n",
        "# grad = tape.gradient(loss, my_vars)\n",
        "# grad['b']\n",
        "\n",
        "\n",
        "# 모델에 대한 그래디언트\n",
        "# tf.Module의 모든 서브 클래스는 Module.trainable_variables 속성에서 변수를 집계하므로 간단하게 그래디언트 계산 가능\n",
        "# layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "# x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     # Forward pass\n",
        "#     y = layer(x)\n",
        "#     loss = tf.reduce_mean(y ** 2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "# grad = tape.gradient(loss, layer.trainable_variables)\n",
        "# for var, g in zip(layer.trainable_variables, grad):\n",
        "#     print(f'{var.name}, shape: {g.shape}')\n",
        "\n",
        "\n",
        "# 테이프의 감시 대상 제어\n",
        "# 테이프는 역방향 패스의 그래디언트를 계산하기 위해 정방향 패스에 기록할 연산을 알아야 함\n",
        "# 테이프는 중간 출력에 대한 참조를 보유하므로 불필요한 연산을 기록하지 않음\n",
        "# 가장 일반적인 사용 사례는 모든 모델의 훈련 가능한 변수에 대해 손실의 그래디언트를 계산\n",
        "# x0 = tf.Variable(3.0, name='x0')\n",
        "# x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor\n",
        "# x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     y = (x0 ** 2)  + (x1 ** 2) + (x2 ** 2)\n",
        "\n",
        "# grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "# for g in grad:\n",
        "#     print(g)\n",
        "\n",
        "# GradientTape.watched_variables 메서드를 사용해 테이프에서 감시 중인 변수 나열\n",
        "# [var.name for var in tape.watched_variables()]\n",
        "\n",
        "# tf.Tensor에 대한 그래디언트를 기록하려면 GradientTape.watch(x) 호출\n",
        "# x = tf.constant(3.0)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     tape.watch(x)\n",
        "#     y = x ** 2\n",
        "\n",
        "# dy_dx = tape.gradient(y, x)\n",
        "# print(dy_dx.numpy())\n",
        "\n",
        "# tf.Variable을 감시하는 기본 동작을 비활성화하려면, 그래디언트 테이프를 만들 때 watch_accessed_variables=False를 설정\n",
        "# x0 = tf.Variable(0.0)\n",
        "# x1 = tf.Variable(10.0)\n",
        "\n",
        "# with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "#     tape.watch(x1)\n",
        "#     y0 = tf.math.sin(x0)\n",
        "#     y1 = tf.nn.softplus(x1)\n",
        "#     y = y0 + y1\n",
        "#     ys = tf.reduce_sum(y)\n",
        "\n",
        "# grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "# print(\"dy/dx0:\", grad['x0'])\n",
        "# print(\"dy/dx1:\", grad['x1'].numpy())\n",
        "\n",
        "\n",
        "# 중간 결과\n",
        "# tf.GradientTape 컨텍스트 내에서 계산된 중간값과 관련해 출력의 그래디언트 요청 가능\n",
        "# x = tf.constant(3.0)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     tape.watch(x)\n",
        "#     y = x * x\n",
        "#     z = y * y\n",
        "\n",
        "# print(tape.gradient(z, y).numpy())\n",
        "\n",
        "# 동일한 계산에 대해 여러 그래디언트를 계산하려면 persistent=True 그래디언트 테이프를 생성\n",
        "# 이러면 테이프 객체가 가비지 수집될 때 리소스가 해제되면 gradient 메서드를 여러 번 호출 가능\n",
        "# x = tf.constant([1, 3.0])\n",
        "# with tf.GradientTape(persistent=True) as tape:\n",
        "#     tape.watch(x)\n",
        "#     y = x * x\n",
        "#     z = y * y\n",
        "\n",
        "# print(tape.gradient(z, x).numpy())\n",
        "# print(tape.gradient(y, x).numpy())\n",
        "\n",
        "# Drop the reference to the tape\n",
        "# del tape\n",
        "\n",
        "\n",
        "# 성능에 대한 참고 사항\n",
        "# 그래디언트 테이프 컨텍스트 내에서 연산을 수행하는 것과 관련해 작은 오버헤드가 있음\n",
        "# 대부분의 Eager 실행에는 상당한 비용이 들지 않지만, 필요한 경우에만 테이프 컨텍스트를 사용해야 함\n",
        "# 그래디언트 테이프는 메모리를 사용하여 역방향 패스 동안 사용하기 위해 입력 및 출력을 포함한 중간 결과 저장\n",
        "# 효율성을 위해 일부 연산은 중간 결과를 유지할 필요가 없으며 정방향 패스 동안 정리됨\n",
        "# 그러나, 테이프에서 persistent=True를 사용하면 아무것도 삭제되지 않으며 최대 메모리 사용량이 높아짐\n",
        "\n",
        "\n",
        "# 스칼라가 아닌 대상의 그래디언트\n",
        "# x = tf.Variable(2.0)\n",
        "# with tf.GradientTape(persistent=True) as tape:\n",
        "#     y0 = x ** 2\n",
        "#     y1 = 1 / x\n",
        "\n",
        "# print(tape.gradient(y0, x).numpy())\n",
        "# print(tape.gradient(y1, x).numpy())\n",
        "# print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())\n",
        "\n",
        "# x = tf.Variable(2.)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     y = x * [3., 4.]\n",
        "\n",
        "# print(tape.gradient(y, x).numpy())\n",
        "\n",
        "# 요소별 계산의 경우, 각 요소가 독립적이므로 합의 그래디언트는 입력 요소와 관련해 각 요소의 미분 제공\n",
        "# x = tf.linspace(-10.0, 10.0, 200 + 1)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     tape.watch(x)\n",
        "#     y = tf.nn.sigmoid(x)\n",
        "\n",
        "# dy_dx = tape.gradient(y, x)\n",
        "\n",
        "# plt.plot(x, y, label='y')\n",
        "# plt.plot(x, dy_dx, label='dy/dx')\n",
        "# plt.legend()\n",
        "# _ = plt.xlabel('x')\n",
        "\n",
        "\n",
        "# 흐름 제어\n",
        "# x = tf.constant(1.0)\n",
        "# v0 = tf.Variable(2.0)\n",
        "# v1 = tf.Variable(2.0)\n",
        "\n",
        "# with tf.GradientTape(persistent=True) as tape:\n",
        "#     tape.watch(x)\n",
        "#     if x > 0.0:\n",
        "#         result = v0\n",
        "#     else:\n",
        "#         result = v1 ** 2\n",
        "\n",
        "# dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "# print(dv0)\n",
        "# print(dv1)\n",
        "\n",
        "# dx = tape.gradient(result, x)\n",
        "# print(dx)\n",
        "\n",
        "\n",
        "# Gradient가 None을 반환하는 경우\n",
        "# target이 source와 연결되어 있지 않으면 gradient(target, source)가 None을 반환\n",
        "# x = tf.Variable(2.)\n",
        "# y = tf.Variable(3.)\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     z = y * y\n",
        "# print(tape.gradient(z, x))\n",
        "\n",
        "## 1. 변수를 텐서로 대체\n",
        "# 테이프의 감시 대상 제어 섹션에서 테이프가 자동으로 tf.Variable을 감시하지만 tf.Tensor는 감시하지 않음\n",
        "# 한 가지 일반적인 오류는 Variable.assign를 사용해 tf.Variable를 업데이트하는 대신 실수로 tf.Tensor로 대체하는 것\n",
        "# x = tf.Variable(2.0)\n",
        "\n",
        "# for epoch in range(2):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         y = x + 1\n",
        "#     print(f\"{type(x).__name__}: {tape.gradient(y, x)}\")\n",
        "#     x = x + 1                  # This should be 'x.assign_add(1)'\n",
        "\n",
        "## 2. TensorFlow 외부에서 계산\n",
        "# 계산에서 TensorFlow를 종료하면 테이프가 그래디언트 경로를 기록할 수 없음\n",
        "# x = tf.Variable([[1.0, 2.0],\n",
        "#                  [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     x2 = x ** 2\n",
        "\n",
        "#     # This step is calculated with NumPy\n",
        "#     y = np.mean(x2, axis=0)\n",
        "\n",
        "#     # Like most ops, reduce_mean will cast the NumPy array to a constant tensor using 'tf.convert_to_tensor'\n",
        "#     y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "# print(tape.gradient(y, x))\n",
        "\n",
        "## 3. 정수 또는 문자열을 통해 그래디언트 계산\n",
        "# 정수와 문자열은 구분할 수 없으며, 계산 경로에서 이러한 데이터 유형을 사용하면 그래디언트는 없음\n",
        "# dtype을 지정하지 않으면 실수로 int 상수 또는 변수를 만들기 쉬움\n",
        "# x = tf.constant(10)\n",
        "# with tf.GradientTape() as tape:\n",
        "#     tape.watch(x)\n",
        "#     y = x * x\n",
        "\n",
        "# print(tape.gradient(y, x))\n",
        "\n",
        "## 4. 상태 저장 개체를 통해 그래디언트 계산\n",
        "# 상태가 그래디언트를 중지함\n",
        "# 상태 저장 객체에서 읽을 때 테이프는 현재 상태만 볼 수 있으며 현재 상태에 이르게 된 기록은 볼 수 없음\n",
        "# tf.Tensor는 작성된 후에는 변경할 수 없음. 즉, 값은 있지만 상태는 없음\n",
        "# tf.Variable은 내부 상태와 값을 가지며, 변수를 사용하면 상태를 읽음\n",
        "# 변수를 사용하여 그래디언트를 계산하는 것이 일반적이지만, 변수의 상태는 그래디언트 계산이 더 멀리 돌아가지 않도록 차단\n",
        "# 마찬가지로, tf.data.Dataset 반복기와 tf.queue는 상태 저장이며 이들을 통과하는 텐서의 모든 그래디언트는 중지\n",
        "# x0 = tf.Variable(3.0)\n",
        "# x1 = tf.Variable(0.0)\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     # Update x1 = x1 + x0\n",
        "#     x1.assign_add(x0)\n",
        "#     # The tape starts recording from x1\n",
        "#     y = x1 ** 2\n",
        "\n",
        "# print(tape.gradient(y, x0))\n",
        "\n",
        "\n",
        "# 그래디언트가 등록되지 않음\n",
        "# 일부 tf.Operation는 미분 불가능한 것으로 등록되어 None 반환\n",
        "# 그래디언트가 등록되지 않은 float op를 통해 그래디언트를 얻고자 시도하면 테이프가 자동으로 None을 반환하는 대신 오류 발생\n",
        "# image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "# delta = tf.Variable(0.1)\n",
        "\n",
        "# with tf.GradientTape() as tape:\n",
        "#     new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "# try:\n",
        "#     print(tape.gradient(new_image, [image, delta]))\n",
        "#     assert False\n",
        "# except LookupError as e:\n",
        "#     print(f\"{type(e).__name__}: {e}\")\n",
        "\n",
        "\n",
        "# None 대신 0\n",
        "# 연결되지 않은 그래디언트의 경우 None 대신 0을 가져오는 것이 편리한 경우가 존재\n",
        "# 연결되지 않은 그래디언트가 있을 때, unconnected_gradients 인수를 사용해 반환할 항목 결정 가능\n",
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    z = y ** 2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ]
}