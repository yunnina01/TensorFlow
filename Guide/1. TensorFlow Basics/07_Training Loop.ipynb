{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWx9nUzmBs0W",
        "outputId": "39c88a0d-4168-4bb3-c46f-5ac02bf8839f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "201\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 9.6013\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.9076\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8072\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.6036\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9084\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5036\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2659\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1252\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0413\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9908\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b5986d44f0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 기본 훈련 루프\n",
        "\n",
        "# 머신러닝 문제 해결\n",
        "# 1. 훈련 데이터 얻기\n",
        "# 2. 모델 정의\n",
        "# 3. 손실 함수 정의\n",
        "# 4. 훈련 데이터를 실행해 이상적인 값에서 손실 계싼\n",
        "# 5. 손실에 대한 기울기를 계산하고 최적화 프로그램을 사용해 데이터에 맞게 변수 조정\n",
        "# 6. 결과 평가\n",
        "\n",
        "# 가장 기본적인 머신러닝 문제인 W(가중치) 및 b(바이어스)의 두 가지 변수가 있는 간단한 선형 모델 f(x) = x * W + b 개발\n",
        "\n",
        "\n",
        "# 데이터\n",
        "# 지도 학습은 입력(일반적으로 x로 표시)과 출력(y로 표시, 종종 레이블이라고 함)을 사용\n",
        "# 목표는 입력에서 출력 값을 예측할 수 있도록 쌍을 이룬 입력과 출력에서 학습\n",
        "# TensorFlow에서 데이터의 각 입력은 거의 항상 텐서로 표현되며(종종 벡터임), 지도 학습에서 출력(또는 예측하려는 값)도 텐서\n",
        "\n",
        "# 다음은 선을 따라 점에 가우시안(정규 분포) 노이즈를 추가하여 합성된 데이터\n",
        "import tensorflow as tf\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "# The actual line\n",
        "# TRUE_W = 3.0\n",
        "# TRUE_B = 2.0\n",
        "\n",
        "# NUM_EXAMPLES = 201\n",
        "\n",
        "# A vector of random x values\n",
        "# x = tf.linspace(-2, 2, NUM_EXAMPLES)\n",
        "# x = tf.cast(x, tf.float32)\n",
        "\n",
        "# def f(x):\n",
        "#     return x * TRUE_W + TRUE_B\n",
        "\n",
        "# Generate some noise\n",
        "# noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "# y = f(x) + noise\n",
        "\n",
        "# Plot all the data\n",
        "# plt.plot(x, y, '.')\n",
        "# plt.show()\n",
        "\n",
        "# 텐서는 일반적으로 배치 또는 입력과 출력이 함께 쌓인 그룹의 형태로 수집\n",
        "# 일괄 처리는 몇 가지 훈련 이점을 제공할 수 있으며, 가속기 및 벡터화된 계산에서 잘 동작\n",
        "# 데이터세트가 얼마나 작은지 고려할 때 전체 데이터세트를 단일 배치로 처리 가능\n",
        "\n",
        "\n",
        "# 모델 정의\n",
        "# tf.Module을 사용해 변수와 계산을 캡슐화\n",
        "# class MyModel(tf.Module):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         # Initialize the weights to '5.0' and the bias to '0.0'\n",
        "#         # In practice, these should be randomly initialized\n",
        "#         self.w = tf.Variable(5.0)\n",
        "#         self.b = tf.Variable(0.0)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         return self.w * x + self.b\n",
        "\n",
        "# model = MyModel()\n",
        "\n",
        "# print(\"Variables:\", model.variables)\n",
        "# assert model(3.0).numpy() == 15.0\n",
        "\n",
        "## 손실 함수 정의\n",
        "# 손실 함수는 주어진 입력에 대한 모델의 출력이 목표 출력과 얼마나 잘 일치하는지 측정\n",
        "# 목표는 훈련 중 이러한 차이를 최소화하는 것\n",
        "\n",
        "# This compute a single loss value for an entire batch\n",
        "# def loss(target_y, predicted_y):\n",
        "#     return tf.reduce_mean(tf.square(target_y - predicted_y))\n",
        "\n",
        "# 모델의 예측을 빨간색, 훈련 데이터를 파란색으로 플롯하여 손실값을 시각화\n",
        "# plt.plot(x, y, '.', label='Data')\n",
        "# plt.plot(x, f(x), label='Ground truth')\n",
        "# plt.plot(x, model(x), label='Predictions')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# print(\"Current loss: %1.6f\" % loss(y, model(x)).numpy())\n",
        "\n",
        "## 훈련 루프 정의\n",
        "# 훈련 루프는 순서대로 다음 작업을 반복적으로 수행하는 것으로 구성\n",
        "# 1. 모델을 통해 입력 배치를 전송하여 출력 생성\n",
        "# 2. 출력을 출력(또는 레이블)과 비교하여 손실 계산\n",
        "# 3. 그래디언트 테이프를 사용해 그래디언트 찾기\n",
        "# 4. 해당 그래디언트로 변수 최적화\n",
        "\n",
        "# 다음은 경사 하강법을 사용한 모델 훈련 방식\n",
        "# Given a callable model, inputs, outputs, and a learning rate...\n",
        "# def train(model, x, y, learning_rate):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         # Trainable variables are automatically tracked by GradientTape\n",
        "#         current_loss = loss(y, model(x))\n",
        "\n",
        "#     # Use GradientTape to calculate the gradients with respect to W and b\n",
        "#     dw, db = tape.gradient(current_loss, [model.w, model.b])\n",
        "\n",
        "#     # Subtract the gradient scaled by the learning rate\n",
        "#     model.w.assign_sub(learning_rate * dw)\n",
        "#     model.b.assign_sub(learning_rate * db)\n",
        "\n",
        "# model = MyModel()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "# weights = []\n",
        "# biases = []\n",
        "# epochs = range(10)\n",
        "\n",
        "# Define a training loop\n",
        "# def report(model, loss):\n",
        "#     return f\"W = {model.w.numpy():1.2f}, b = {model.b.numpy():1.2f}, loss = {loss:2.5f}\"\n",
        "\n",
        "# def training_loop(model, x, y):\n",
        "#     for epoch in epochs:\n",
        "#         # Update the model with the single giant batch\n",
        "#         train(model, x, y, learning_rate=0.1)\n",
        "\n",
        "#         # Track this before I update\n",
        "#         weights.append(model.w.numpy())\n",
        "#         biases.append(model.b.numpy())\n",
        "#         current_loss = loss(y, model(x))\n",
        "\n",
        "#         print(f\"Epoch {epoch:2d}:\")\n",
        "#         print(\"    \", report(model, current_loss))\n",
        "\n",
        "# 훈련 수행\n",
        "# current_loss = loss(y, model(x))\n",
        "\n",
        "# print(f\"Starting:\")\n",
        "# print(\"    \", report(model, current_loss))\n",
        "\n",
        "# training_loop(model, x, y)\n",
        "\n",
        "# 시간이 지남에 따른 가중치 전개 양상 시각화\n",
        "# plt.plot(epochs, weights, label='Weighs', color=colors[0])\n",
        "# plt.plot(epochs, [TRUE_W] * len(epochs), '--',\n",
        "#          label='True weight', color=colors[0])\n",
        "# plt.plot(epochs, biases, label='Bias', color=colors[1])\n",
        "# plt.plot(epochs, [TRUE_B] * len(epochs), '--',\n",
        "#          label='True bias', color=colors[1])\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# 훈련된 모델 성능 시각화\n",
        "# plt.plot(x, y, '.', label='Data')\n",
        "# plt.plot(x, f(x), label='Ground truth')\n",
        "# plt.plot(x, model(x), label='Predictions')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# print(\"Current loss: %1.6f\" % loss(model(x), y).numpy())\n",
        "\n",
        "\n",
        "# 같은 해결방법이지만, Keras를 사용한 경우\n",
        "# tf.keras.Model을 하위 클래스화하면 모델 정의는 정확히 같게 보임\n",
        "# Keras 모델은 궁극적으로 모듈에서 상속\n",
        "class MyModelKeras(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.w = tf.Variable(5.0)\n",
        "        self.b = tf.Variable(0.0)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.w * x + self.b\n",
        "\n",
        "# keras_model = MyModelKeras()\n",
        "\n",
        "# Reuse the training loop with a Keras model\n",
        "# training_loop(keras_model, x, y)\n",
        "\n",
        "# You can also save a checkpoint using Keras's built-in support\n",
        "# keras_model.save_weights(\"my_checkpoint\")\n",
        "\n",
        "# 모델을 생성할 때마다 새로운 훈련 루프를 작성하는 대신 Keras의 내장 기능을 사용 가능\n",
        "# Python 훈련 루프를 작성하거나 디버깅하지 않으려는 경우 유용\n",
        "# Keras의 내장 기능을 사용하려면, model.compile()을 사용하여 매개변수를 설정하고 model.fit()을 사용해 훈련\n",
        "keras_model = MyModelKeras()\n",
        "\n",
        "# compile sets the training parameters\n",
        "keras_model.compile(\n",
        "    # By default, fit() uses tf.function().\n",
        "    # You can turn that off for debugging, but it is on now\n",
        "    run_eagerly=False,\n",
        "\n",
        "    # Using a built-in optimizer, configuring as an object\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "\n",
        "    # Keras comes with built-in MSE error\n",
        "    # However, you could use the loss function\n",
        "    loss=tf.keras.losses.mean_squared_error,\n",
        ")\n",
        "\n",
        "# Keras fit 배치 데이터 또는 전체 데이터세트를 NumPy 배열로 예상\n",
        "# NumPy 배열은 배치로 분할되며, 기본 배치 크기는 32\n",
        "# 이 경우, 직접 작성한 루프의 동작과 일치시키기 위해 x를 크기 1000의 단일 배치로 전달\n",
        "print(x.shape[0])\n",
        "keras_model.fit(x, y, epochs=10, batch_size=1000)"
      ]
    }
  ]
}